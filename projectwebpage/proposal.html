<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | Fall 2019: CS 6476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">
  

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
font-family: Arial, Helvetica, sans-serif
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header", style="text-align: center">

<!-- Title and Name --> 
<h1>Real-time human emotions detection from videos</h1> 
<span style="font-size: 18px; line-height: 1.5em;">Chitwan Kaudan, Ling Yiu Ku, Raghav Apoorv, Tien T Dinh</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2019 | CS 6476 Computer Vision Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Institute of Technology</span>
<hr>
</div>

<!-- Goal -->
<h3>Problem Statement</h3>
Facial expressions are a form of nonverbal communication. Various studies have been done for the classification of these facial expressions on images. There is strong evidence for the universal facial expressions of eight emotions which include: <i>neutral, happiness, sadness, anger, contempt, disgust, fear, and surprise</i>. In our project, we would like to extend existing emotion detection work to visualize changes in an individual’s emotions over time. Adding a time component opens up the possibility of understanding the relatives changes in emotions as reactions to some real world stimulus. 
<br><br>
One application would be real time sentiment analysis for the state of the union address. Instead of typical sentiment analysis that uses NLP on tweets or other direct surveying methods, our approach could extract emotions from video in real time, display smoothed changes in emotions over time and even conduct a variety of traditional time series analysis such as change detection. Another application could be determining when and what types of ads to serve during a youtube video so the mood of the ad matches the mood of the content. We believe a program that detects emotion from facial expressions over time would be widely applicable. 
<hr>
<!-- Approach text -->
<h3>Approach</h3>

<!-- Approach figure --> 
<div style="text-align: left;">
<img style="height: 600px;" alt="" src="approach.png">
</div>
<hr>
<!-- Datasets -->
<h3>Experiments & Design</h3>
<h4>Datasets</h4>
We are using a combination of human images and animated images to train our deep learning model, and several publicly available videos and self-recorded as video inputs after the minimum viable product is built.
<ul type="disc">
    <li>
      <a href="https://faces.mpdl.mpg.de/imeji/">FACES</a> - dataset contains images of 171 faces, displaying 2 sets of 6 facial expressions: neutrality, sadness, disgust, fear, anger, and happiness. Resulting in a total of 2052 images
    </li>
    <li>
      <a href="http://app.visgraf.impa.br/database/faces/details/6/">FacesDB - VISGRAF faces database</a> - dataset contains images of 38 individuals corresponding to six universal facial expressions and other expressions referring to 5 samples containing mouth and eyes open and / or closed, summarizing 532 samples in total
    </li>
    <li>
      <a href="http://www.pitt.edu/~emotion/ck-spread.htm">Cohn-Kanade AU-Coded Expression Database</a> - dataset contains images of 97 individuals corresponging to a total of 486 samples
    </li>
    <li>
      <a href="https://zenodo.org/record/3451524#.XZoY3y2ZO_s">JAFFE database</a> - dataset contains 10 Japanese female models, displaying 7 facial expressions constituting a total of 213 images
    </li>
    <li>
      <a href="https://mmifacedb.eu">MMI database</a> - dataset contains images and videos of 75 subjects, summarizing 2900 samples in total
    </li>
    <li>
      <a href="http://www.consortium.ri.cmu.edu/data/ck/">Cohn-Kanade (CK and CK+)</a> - dataset contains images of 220 individuals, summarizing 1079 samples
    </li>
    <li>
      <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">FER dataset</a> - dataset contains 7178 samples containing the 6 facial expressions
    </li>
    <li>
      <a href="https://computervisiononline.com/dataset/1105138659">AFEW videos</a> - dataset contains 700 images containing 6 facial expressions
    </li>
</ul>
<h4>Anticipated Implementation Overview</h4>
Note: We anticipate we will be able to implement everything in Python/Tensorflow.
<br><br>
<table style="width:90%">
  <table border="1|0">
    <tr>
      <th style="padding-right: 50px">Experiment</th>
      <th style="width: 50%">Methodology</th>
      <th>Reference</th>
    </tr>
    <tr>
      <td>Facial Detection</td>
      <td>Facial detection is an interesting and heavily researched field; however, it is not the main focus of our project. We plan to use David Sandberg’s FaceNet which is a Python/Tensorflow implementation of the MTCNN method for detecting faces from all angles. This method provides state of the art accuracy on the Labeled Faces in the Wild (LFW) dataset of 99.6%.
    </td>
      <td>
        <ul type="disc">
        <li><a href="https://arxiv.org/abs/1503.03832">Paper</a></li>
        <li><a href="https://github.com/davidsandberg/facenet">Code</a></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>Feature Extraction</td>
      <td>Feature extraction will be one of the focal points of our project, so we will be implementing a few methods of feature extraction and comparing the results and the computational cost of each method. Below are a list of methods we would like to try but this list is subject to change as we learn more about feature extraction. We may also run into computational limits and implementing a particular method is not possible for us.<p>We are aware than some of the traditional feature extraction algorithm implementations are available in OpenCV; however, we would like to implement these ourselves to gain a better understanding of the algorithms.</td>
      <td>
        <ul type="disc">
          <li>
            Geometric methods
            <ul type="square">
              <li>Landmark Euclidean Distance (LMED) <a href="https://dl.acm.org/citation.cfm?id=326498">Paper </a>
              </li>
              <li>
            Eccentricity and linear features (Model S5) <a href="http://www.portointeractivecenter.org/site/wp-content/uploads/Real-Time-Emotion-Recognition_a-Novel-Method-for-Geometrical-Facial-Features-Extraction1.pdf">Paper </a>
              </li>
          </ul>
          </li>
          <li>
            Appearance based methods
            <ul type="square">
              <li>Histogram of Oriented Gradients (HoG) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4628009/">Paper </a>
              </li>
              <li>
            Local Binary Patterns (LBP) <a href=" https://liris.cnrs.fr/Documents/Liris-5004.pdf">Paper </a>
              </li>
          </ul>
          </li>
        </ul>
        
      </td>
    </tr>
    <tr>
      <td>Learned Classification</td>
      <td>We will be training own classifier although we will rely on some pretty high level tools (i.e. Tensorflow). We want to try a few classification models and compare/contrast their accuracy and runtimes.</td>
      <td>
        <ul type="disc">
          <li>
            Weighted Random Forest Classifier (WRF) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308562/">Paper </a>
          </li>
          <li>
            Support Vector Machine (SVM) <a href="https://dl.acm.org/citation.cfm?id=3264989">Paper </a>
          </li>
          <li>
            Convolutional Neural Network (CNN) / DenseNet <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8337514">Paper </a>
          </li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>Output Modeling/Visualization</td>
      <td>
            As far as we are aware, output modeling/visualization to create time series plots will be our completely novel contribution in this project. Ideally, we would like to generate these plots in close to real time and build an online visualization tool. We anticipate we may have to do some post-processing (i.e. smoothing) on our classification results to build good visualizations but the exact details of the post processing will depend on our classification results.
          <p>
            Additionally, we would also like to run some multivariate change detection algorithms to see if changes our model predicts correspond to changes in the real world scene. 
      </td>
      <td>
      </td>
    </tr>
  </table>
<hr>
<!-- Success -->
<h4>What does success look like</h4>
Through our experimentation we will be trading off accuracy for computational viability, so we would like our end product to be able to run in real time and be as accurate as possible. Per our idea on applying this technology in union address sentiment analysis, we plan to test on several self-recorded videos featuring ourselves reacting to a speech.
<p>
  The final product will showcase a real time emotion detector, which would plot the facial expressions and denote the changes in the emotion. We would be able to identify with high accuracy when the sentiment of the conversation has changed looking at the expressions of the person. 
<p>
  This would enable us to have impact in various sectors, namely - more enhanced conversations for people with visual impairment. They would get a real time metric of the emotions of the person listening to their conversation, and as when the expressions change would have a more immersive effect on the conversations.
<p>
  Secondly, we would also test the same on the Union Address, picking up the sentiments of the address from the facial expressions of both parties. This would enable us to determine the impact (happy/sad - positive/negative) it has on the audience.
<p>
  Thirdly, we will be recodring ourselves watching various videos and our reactions to a speech, this would be the final test determining how accurate our product would be. Being able to detect the sentiments and changes in the emotions overtime during a conversation in real time would be our final aim. 
<br><br>

<hr>
<!-- Uncertainties -->
<h4>Uncertainties in potential outcomes</h4>
<i>We may not be able to achieve state of the art accuracies.</i>
<ul type="disc">
    <li>Most papers we found about feature extraction methods describe in detail their mathematical approach but do not make their source code publically available. Our implementation will be our interpretation of these methods and may not achieve the same state of the art results described in the originating papers.</li>
</ul>
<i>Emotion predictions might be discontinuous across frames.</i>
<ul type="disc">
    <li>Depending on the model and the input video, it is possible that the top emotion prediction is different across consecutive frames. For example, in five consecutive frames, the output emotions might be “happy, happy, happy, sad, happy”. In this case, frame 4 might be a detection error. We will consider using exponential smoothing or vote-and-verify strategy to remove these noises. </li>
</ul>




  </p>

  <hr>
  <footer> 
  </footer>

</div>

<br><br>

</body></html>